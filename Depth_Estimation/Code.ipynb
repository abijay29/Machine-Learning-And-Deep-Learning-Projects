{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":96480,"databundleVersionId":11466546,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Installing the Required Dependencies ","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating the Directories","metadata":{}},{"cell_type":"code","source":"# Path to data\nTRAIN_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/training\"\nVAL_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/validation\"\nTEST_DIR = \"/kaggle/input/depth-estimation/competition-data/competition-data/testing\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to convert images to CSV \ndef images_to_csv_with_metadata(image_folder, output_csv):\n    # Initialize an empty list to store image data and metadata\n    data = []\n    \n    # Loop through all images in the folder\n    for idx, filename in enumerate(sorted(os.listdir(image_folder))):\n        if filename.endswith(\".png\"):\n            filepath = os.path.join(image_folder, filename)\n            \n            # Read the image\n            image = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n            image = cv2.resize(image, (128, 128))\n            image = image / 255.\n            image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-6)\n            image = np.uint8(image * 255.)\n            \n            # Flatten the image into a 1D array\n            image_flat = image.flatten()\n            \n            # Add ID, ImageID (filename), and pixel values\n            row = [idx, filename] + image_flat.tolist()\n            data.append(row)\n    \n    # Create a DataFrame\n    num_columns = len(data[0]) - 2 if data else 0\n    column_names = [\"id\", \"ImageID\"] + [indx for indx in range(num_columns)]\n    df = pd.DataFrame(data, columns=column_names)\n    \n    # Save to CSV\n    df.to_csv(output_csv, index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Dataset Creation","metadata":{}},{"cell_type":"code","source":"# Custom Dataset Class\nclass DepthEstimationDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.rgb_dir = os.path.join(root_dir, 'images')\n        self.depth_dir = os.path.join(root_dir, 'depths') if 'depths' in os.listdir(root_dir) else None\n        \n        # Get all RGB image files\n        self.rgb_files = [f for f in os.listdir(self.rgb_dir) \n                         if os.path.isfile(os.path.join(self.rgb_dir, f)) and \n                         (f.endswith('.jpg') or f.endswith('.png'))]\n        \n        if len(self.rgb_files) == 0:\n            raise ValueError(f\"No image files found in {self.rgb_dir}\")\n        \n        self.transform = transform or transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n    def __len__(self):\n        return len(self.rgb_files)\n    \n    def __getitem__(self, idx):\n        # Load RGB image\n        img_name = self.rgb_files[idx]\n        img_path = os.path.join(self.rgb_dir, img_name)\n        image = Image.open(img_path).convert('RGB')\n        \n        # Apply transforms to image\n        if self.transform:\n            image = self.transform(image)\n        \n        # For test dataset, we might not have depth maps\n        depth_map = None\n        if self.depth_dir:\n            # Assuming depth maps have the same filename but possibly different extension\n            depth_name = os.path.splitext(img_name)[0]\n            potential_depth_files = [\n                f\"{depth_name}.png\",\n                f\"{depth_name}.jpg\",\n                f\"{depth_name}.npy\"\n            ]\n            \n            for depth_file in potential_depth_files:\n                depth_path = os.path.join(self.depth_dir, depth_file)\n                if os.path.exists(depth_path):\n                    # Handle different depth map formats\n                    if depth_file.endswith('.npy'):\n                        depth_map = torch.from_numpy(np.load(depth_path)).float()\n                    else:\n                        depth_img = Image.open(depth_path).convert('L')  # Grayscale\n                        depth_map = transforms.ToTensor()(depth_img)\n                    break\n        \n        return {\n            'image': image,\n            'depth': depth_map,\n            'filename': img_name\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Augmentation transforms\ndef get_training_augmentation():\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    return transform","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining the Model","metadata":{}},{"cell_type":"code","source":"# U-Net Model \nclass DoubleConv(nn.Module):\n    \"\"\"Double convolution block with batch normalization\"\"\"\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n       \n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        \n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1, bilinear=True, features=[64, 128, 256, 512]):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        # Encoder path\n        self.inc = DoubleConv(n_channels, features[0])\n        self.down1 = Down(features[0], features[1])\n        self.down2 = Down(features[1], features[2])\n        self.down3 = Down(features[2], features[3])\n        factor = 2 if bilinear else 1\n        self.down4 = Down(features[3], features[3] * 2 // factor)\n        \n        # Decoder path\n        self.up1 = Up(features[3] * 2, features[3] // factor, bilinear)\n        self.up2 = Up(features[3], features[2] // factor, bilinear)\n        self.up3 = Up(features[2], features[1] // factor, bilinear)\n        self.up4 = Up(features[1], features[0], bilinear)\n        self.outc = OutConv(features[0], n_classes)\n\n    def forward(self, x):\n        # Encoder\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        \n        # Decoder\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        \n        return logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating Loss functions","metadata":{}},{"cell_type":"code","source":"# Custom Loss Functions\nclass GradientLoss(nn.Module):\n    def __init__(self):\n        super(GradientLoss, self).__init__()\n        self.sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).reshape(1, 1, 3, 3)\n        self.sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).reshape(1, 1, 3, 3)\n        \n    def forward(self, pred, target):\n        if self.sobel_x.device != pred.device:\n            self.sobel_x = self.sobel_x.to(pred.device)\n            self.sobel_y = self.sobel_y.to(pred.device)\n            \n        # Expand dimensions for depthwise convolution\n        pred_expanded = pred.expand(-1, 1, -1, -1)\n        target_expanded = target.expand(-1, 1, -1, -1)\n        \n        # Compute gradients using Sobel operator\n        pred_grad_x = F.conv2d(pred_expanded, self.sobel_x, padding=1)\n        pred_grad_y = F.conv2d(pred_expanded, self.sobel_y, padding=1)\n        target_grad_x = F.conv2d(target_expanded, self.sobel_x, padding=1)\n        target_grad_y = F.conv2d(target_expanded, self.sobel_y, padding=1)\n        \n        # Compute L1 loss on gradients\n        loss_grad_x = F.l1_loss(pred_grad_x, target_grad_x)\n        loss_grad_y = F.l1_loss(pred_grad_y, target_grad_y)\n        \n        return loss_grad_x + loss_grad_y\n\nclass DepthLoss(nn.Module):\n    def __init__(self):\n        super(DepthLoss, self).__init__()\n        self.l1_loss = nn.L1Loss()\n        self.mse_loss = nn.MSELoss()\n        self.gradient_loss = GradientLoss()\n        \n    def forward(self, pred, target):\n        # Scale weights for different loss components\n        l1 = self.l1_loss(pred, target) * 0.5\n        mse = self.mse_loss(pred, target) * 0.3\n        gradient = self.gradient_loss(pred, target) * 0.2\n        \n        return l1 + mse + gradient","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom collate function to handle None values\ndef custom_collate_fn(batch):\n   \n    filtered_batch = []\n    for item in batch:\n        if item['depth'] is None:\n           \n            item['depth'] = torch.zeros((1, item['image'].shape[1], item['image'].shape[2]), dtype=torch.float32)\n        filtered_batch.append(item)\n    \n    return torch.utils.data.dataloader.default_collate(filtered_batch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Dataset ","metadata":{}},{"cell_type":"code","source":"# Initialize datasets and dataloaders\ndef initialize_dataloaders(batch_size=8):\n    # Training dataset with augmentation\n    train_transform = get_training_augmentation()\n    train_dataset = DepthEstimationDataset(TRAIN_DIR, transform=train_transform)\n    \n    # Validation and test datasets without augmentation\n    val_dataset = DepthEstimationDataset(VAL_DIR)\n    test_dataset = DepthEstimationDataset(TEST_DIR)\n    \n    # Creating dataloaders with custom collate function\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=4, \n        pin_memory=True,\n        collate_fn=custom_collate_fn  \n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True,\n        collate_fn=custom_collate_fn \n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True,\n        collate_fn=custom_collate_fn  \n    )\n    \n    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"code","source":"# Training function\ndef train_model(model, train_loader, val_loader, num_epochs=20, batch_size=32):\n    # Define loss function, optimizer and scheduler\n    criterion = DepthLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n    )\n    \n    # Initialize variables\n    best_val_rmse = float('inf')\n    train_losses = []\n    val_losses = []\n    train_rmses = []\n    val_rmses = []\n    \n    # Training loop\n    print(\"Starting training...\")\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n        train_rmse = 0.0\n        \n        for batch_idx, batch in enumerate(train_loader):\n            images = batch['image'].to(device)\n            depths = batch['depth'].to(device)\n            \n            # Clear gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Compute loss\n            loss = criterion(outputs, depths)\n            \n            # Compute RMSE for monitoring\n            with torch.no_grad():\n                rmse = torch.sqrt(F.mse_loss(outputs, depths))\n                train_rmse += rmse.item()\n            \n            # Backward pass\n            loss.backward()\n            \n            # Clip gradients to prevent explosion\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            # Update weights\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n            # Printing batch progress every 10 batches\n            if (batch_idx + 1) % 10 == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}, RMSE: {rmse.item():.4f}')\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_rmse = 0.0\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                images = batch['image'].to(device)\n                depths = batch['depth'].to(device)\n                \n                outputs = model(images)\n                loss = criterion(outputs, depths)\n                rmse = torch.sqrt(F.mse_loss(outputs, depths))\n                \n                val_loss += loss.item()\n                val_rmse += rmse.item()\n        \n        # Calculate average metrics\n        train_loss = train_loss / len(train_loader)\n        train_rmse = train_rmse / len(train_loader)\n        val_loss = val_loss / len(val_loader)\n        val_rmse = val_rmse / len(val_loader)\n        \n        # Storing metrics for plotting\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_rmses.append(train_rmse)\n        val_rmses.append(val_rmse)\n        \n        # Updating learning rate\n        scheduler.step(val_rmse)\n        \n        # Printing epoch statistics\n        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}')\n        \n        # Save best model\n        if val_rmse < best_val_rmse:\n            best_val_rmse = val_rmse\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_rmse': val_rmse,\n            }, 'best_unet_model.pth')\n            print(f\"Saved best model with validation RMSE: {best_val_rmse:.4f}\")\n    \n    # Plot training and validation metrics\n    plot_metrics(train_losses, val_losses, train_rmses, val_rmses)\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to plot training metrics\ndef plot_metrics(train_losses, val_losses, train_rmses, val_rmses):\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss vs. Epoch')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(train_rmses, label='Train RMSE')\n    plt.plot(val_rmses, label='Val RMSE')\n    plt.xlabel('Epoch')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.title('RMSE vs. Epoch')\n    \n    plt.tight_layout()\n    plt.savefig('training_metrics.png')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Prdictions CSV","metadata":{}},{"cell_type":"code","source":"# Function to generate predictions\ndef generate_predictions(model, test_loader, device, output_folder=\"predictions\"):\n    \"\"\"Generate depth predictions and save them as PNG files.\"\"\"\n    model.eval()\n    \n    # Create output folder if it doesn't exist\n    os.makedirs(output_folder, exist_ok=True)\n    \n    print(\"Generating predictions...\")\n    with torch.no_grad():\n        for batch in test_loader:\n            images = batch['image'].to(device)\n            filenames = batch['filename']\n            \n            # Generate predictions\n            outputs = model(images)\n            \n            # Process each prediction\n            for i in range(len(outputs)):\n                # Get filename and prediction\n                filename = filenames[i]\n                pred = outputs[i].cpu().numpy().squeeze()\n                \n                # Resize to required size (128x128)\n                pred_resized = cv2.resize(pred, (128, 128))\n                \n                # Normalize prediction\n                pred_norm = (pred_resized - np.min(pred_resized)) / (np.max(pred_resized) - np.min(pred_resized) + 1e-6)\n                pred_uint8 = np.uint8(pred_norm * 255)\n                \n                # Save prediction\n                output_path = os.path.join(output_folder, filename)\n                cv2.imwrite(output_path, pred_uint8)\n    \n    print(f\"Predictions saved to {output_folder}\")\n    return output_folder\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main execution function\ndef main():\n    \n    BATCH_SIZE = 32\n    NUM_EPOCHS = 20\n    \n    # Initializing dataloaders\n    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = initialize_dataloaders(BATCH_SIZE)\n    \n    # Printing basic  dataset stats\n    print(f\"Train dataset size: {len(train_dataset)}\")\n    print(f\"Validation dataset size: {len(val_dataset)}\")\n    print(f\"Test dataset size: {len(test_dataset)}\")\n    \n    # Create model\n    model = UNet(n_channels=3, n_classes=1, bilinear=True).to(device)\n    \n    \n    print(model)\n    \n    # Training the model\n    trained_model = train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n    \n    # Loading best model for predictions\n    checkpoint = torch.load('best_unet_model.pth')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"Loaded best model from epoch {checkpoint['epoch']} with validation RMSE: {checkpoint['val_rmse']:.4f}\")\n    \n    # Generating predictions\n    output_folder = \"data/sample_solution\"\n    predictions_folder = generate_predictions(model, test_loader, device, output_folder)\n    \n    # Converting predictions to CSV\n    predictions_csv = \"predictions.csv\"\n    images_to_csv_with_metadata(predictions_folder, predictions_csv)\n    print(f\"Predictions saved to CSV: {predictions_csv}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}