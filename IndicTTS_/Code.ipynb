{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 95302,
          "databundleVersionId": 11325230,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "id": "tH7xR_vV1Is9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Install & Import Dependencies"
      ],
      "metadata": {
        "id": "yHUbxzKP1Is-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets librosa torch  scikit-learn tqdm evaluate"
      ],
      "metadata": {
        "trusted": true,
        "id": "AuXtdTGG1Is_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import softmax\n",
        "from tqdm import tqdm\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from evaluate import load\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Union"
      ],
      "metadata": {
        "trusted": true,
        "id": "0RrAkg-81ItA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Dataset"
      ],
      "metadata": {
        "id": "WHf9zy2l1ItA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "speech_dataset = load_dataset(\"SherryT997/IndicTTS-Deepfake-Challenge-Data\")\n",
        "train_samples = speech_dataset[\"train\"]\n",
        "test_samples = speech_dataset[\"test\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "RlX3268d1ItA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Pre-trained Model"
      ],
      "metadata": {
        "id": "cWlvFgfB1ItB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
        "extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "audio_model = AutoModelForAudioClassification.from_pretrained(model_name, num_labels=2)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "2yssKB4t1ItB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Balanced Subset of the Dataset"
      ],
      "metadata": {
        "id": "84cTxiND1ItB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique language identifiers\n",
        "language_set = set(train_samples[\"language\"])\n",
        "\n",
        "# Sample 1/4 of the dataset while preserving language distribution\n",
        "subset_indices = []\n",
        "portion = 0.25\n",
        "\n",
        "for lang in language_set:\n",
        "    indices = [idx for idx, val in enumerate(train_samples[\"language\"]) if val == lang]\n",
        "    shuffled_data = train_samples.select(indices).shuffle(seed=42)\n",
        "    num_samples = max(1, int(len(indices) * portion))\n",
        "    subset_indices.extend(indices[:num_samples])\n",
        "\n",
        "# Create a sampled dataset\n",
        "filtered_dataset = train_samples.select(subset_indices)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "AWIv03IH1ItC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "W1XwZ7qE1ItC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Audio"
      ],
      "metadata": {
        "id": "ERsjzAUB1ItC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio(sample):\n",
        "    signal = sample[\"audio\"][\"array\"]\n",
        "    target_length = 32000\n",
        "\n",
        "    # Adjust audio length\n",
        "    if len(signal) < target_length:\n",
        "        signal = np.pad(signal, (0, target_length - len(signal)), mode='constant')\n",
        "    else:\n",
        "        signal = signal[:target_length]\n",
        "\n",
        "    # Extract features\n",
        "    processed_input = extractor(signal, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    sample[\"input_values\"] = processed_input.input_values[0]\n",
        "    sample[\"labels\"] = torch.tensor(sample[\"is_tts\"], dtype=torch.float)\n",
        "    return sample"
      ],
      "metadata": {
        "trusted": true,
        "id": "QGUFcgBG1ItC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply transformation\n",
        "filtered_dataset = filtered_dataset.map(process_audio, remove_columns=[\"audio\", \"text\", \"id\", \"language\", \"is_tts\"])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lp9hJt111ItC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting dataset for training and evaluation"
      ],
      "metadata": {
        "id": "u_IZlG9n1ItC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset for training and evaluation\n",
        "data_splits = filtered_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "1KnM4cD01ItC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Custom Data Collator"
      ],
      "metadata": {
        "id": "gqQHdnHS1ItC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class CustomCollator:\n",
        "    extractor: extractor\n",
        "    padding: Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        inputs = [{\"input_values\": sample[\"input_values\"]} for sample in batch]\n",
        "        padded_batch = self.extractor.pad(inputs, padding=self.padding, return_tensors=\"pt\")\n",
        "        padded_batch[\"labels\"] = torch.tensor([sample[\"labels\"] for sample in batch], dtype=torch.long)\n",
        "        return padded_batch\n",
        "\n",
        "collator = CustomCollator(extractor, padding=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "qfiwvKHU1ItD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Evaluation Metric"
      ],
      "metadata": {
        "id": "5ljA5zue1ItD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation metrics\n",
        "\n",
        "def evaluate_metrics(predictions):\n",
        "    pred_logits = predictions.predictions\n",
        "    pred_probs = softmax(pred_logits, axis=-1)[:, 1]\n",
        "    true_labels = predictions.label_ids\n",
        "\n",
        "    return {\n",
        "        \"roc_auc\": load(\"roc_auc\").compute(prediction_scores=pred_probs, references=true_labels)[\"roc_auc\"]\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "id": "GfJ8wkz91ItD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Defining Training Arguments"
      ],
      "metadata": {
        "id": "oN_vIxAi1ItD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_config = TrainingArguments(\n",
        "    output_dir=\"/output\",\n",
        "    group_by_length=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    save_steps=500,\n",
        "    eval_steps=250,\n",
        "    logging_steps=250,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "7os3jXzv1ItD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the Trainer"
      ],
      "metadata": {
        "id": "6QGlCwZb1ItD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize trainer\n",
        "trainer_instance = Trainer(\n",
        "    model=audio_model,\n",
        "    data_collator=collator,\n",
        "    args=training_config,\n",
        "    compute_metrics=evaluate_metrics,\n",
        "    train_dataset=data_splits[\"train\"],\n",
        "    eval_dataset=data_splits[\"test\"],\n",
        "    tokenizer=extractor\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "rDn6uj5o1ItD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "WsUkmEXj1ItD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_instance.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "FdAsYit_1ItD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Model Performance on Test Data"
      ],
      "metadata": {
        "id": "XcT_ziMg1ItD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference and submission\n",
        "submission_list = []\n",
        "audio_model.eval()\n",
        "for entry in tqdm(test_samples):\n",
        "    entry_id = entry[\"id\"]\n",
        "    input_audio = entry[\"audio\"][\"array\"]\n",
        "    processed_audio = extractor(input_audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    processed_audio = {key: value.to(device) for key, value in processed_audio.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_logits = audio_model(**processed_audio).logits\n",
        "\n",
        "    pred_probs = softmax(pred_logits.cpu().numpy(), axis=-1)\n",
        "    tts_probability = round(pred_probs[0, 1], 3)\n",
        "    submission_list.append([entry_id, tts_probability])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "cIfm5tP51ItD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame(submission_list, columns=[\"id\", \"is_tts\"])\n",
        "submission_df.to_csv(\"./final_submission.csv\", index=False)\n",
        "\n",
        "print(\"Submission file successfully saved!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "c93ET37w1ItD"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}