{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b5539-3806-48d2-ad73-fe798b4ae744",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43ef54-05ec-48e8-9a63-99d4bd7dab48",
   "metadata": {},
   "source": [
    "## Installing the Required Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5afbe-1066-4ca6-8ca7-32668fe1826f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import vgg19\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042748e-001a-415b-92c0-8f45c6d5cd2e",
   "metadata": {},
   "source": [
    "## Data Augmentation and Preprocessing for Super-Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608114ad",
   "metadata": {
    "papermill": {
     "duration": 0.011469,
     "end_time": "2025-04-05T07:08:57.339788",
     "exception": false,
     "start_time": "2025-04-05T07:08:57.328319",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def random_cropping(lr_img, hr_img, crop_size_hr=128, scale=4):\n",
    "    \"\"\"Performs a random crop on the high-resolution image and corresponding low-resolution image.\"\"\"\n",
    "    hr_width, hr_height = hr_img.size\n",
    "    x = random.randint(0, hr_width - crop_size_hr)\n",
    "    y = random.randint(0, hr_height - crop_size_hr)\n",
    "\n",
    "    hr_crop = F.crop(hr_img, y, x, crop_size_hr, crop_size_hr)\n",
    "    lr_crop = F.crop(\n",
    "        lr_img,\n",
    "        y // scale,\n",
    "        x // scale,\n",
    "        crop_size_hr // scale,\n",
    "        crop_size_hr // scale\n",
    "    )\n",
    "    return lr_crop, hr_crop\n",
    "\n",
    "class SRTransform:\n",
    "    \"\"\"Applies data augmentation for Super Resolution tasks including cropping, flipping, rotating, and color jitter.\"\"\"\n",
    "    def __init__(self, crop_size_hr=128, scale=4):\n",
    "        self.crop_size_hr = crop_size_hr\n",
    "        self.scale = scale\n",
    "        self.color_jitter = T.ColorJitter(\n",
    "            brightness=0.1,\n",
    "            contrast=0.1,\n",
    "            saturation=0.1,\n",
    "            hue=0.05\n",
    "        )\n",
    "\n",
    "    def augment(self, lr_img, hr_img):\n",
    "        \"\"\"Applies random horizontal flip, vertical flip, and rotation.\"\"\"\n",
    "        if random.random() > 0.5:\n",
    "            lr_img, hr_img = F.hflip(lr_img), F.hflip(hr_img)\n",
    "        if random.random() > 0.5:\n",
    "            lr_img, hr_img = F.vflip(lr_img), F.vflip(hr_img)\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.choice([90, 180, 270])\n",
    "            lr_img, hr_img = F.rotate(lr_img, angle), F.rotate(hr_img, angle)\n",
    "        return lr_img, hr_img\n",
    "\n",
    "    def __call__(self, lr_img, hr_img):\n",
    "        lr_crop, hr_crop = random_cropping(lr_img, hr_img, self.crop_size_hr, self.scale)\n",
    "        lr_aug, hr_aug = self.augment(lr_crop, hr_crop)\n",
    "\n",
    "        lr_aug = self.color_jitter(lr_aug)\n",
    "        return F.to_tensor(lr_aug), F.to_tensor(hr_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed76dfa",
   "metadata": {
    "papermill": {
     "duration": 7.103157,
     "end_time": "2025-04-05T07:08:57.324992",
     "exception": false,
     "start_time": "2025-04-05T07:08:50.221835",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LowLightSRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir=None, transform_lr=None, transform_hr=None):\n",
    "       \n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_files = sorted(glob.glob(os.path.join(lr_dir, \"*.png\")))\n",
    "        \n",
    "       \n",
    "        if hr_dir is not None:\n",
    "            self.hr_files = sorted(glob.glob(os.path.join(hr_dir, \"*.png\")))\n",
    "        else:\n",
    "            self.hr_files = None\n",
    "        \n",
    "        self.transform_lr = transform_lr\n",
    "        self.transform_hr = transform_hr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path = self.lr_files[idx]\n",
    "        lr_img = Image.open(lr_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform_lr:\n",
    "            lr_img = self.transform_lr(lr_img)\n",
    "\n",
    "        if self.hr_files is not None:\n",
    "            hr_path = self.hr_files[idx]\n",
    "            hr_img = Image.open(hr_path).convert(\"RGB\")\n",
    "            if self.transform_hr:\n",
    "                hr_img = self.transform_hr(hr_img)\n",
    "            return lr_img, hr_img\n",
    "        else:\n",
    "            \n",
    "            return lr_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac67bc-afeb-49eb-a68e-cf79541c69c0",
   "metadata": {},
   "source": [
    "## Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b09aa",
   "metadata": {
    "papermill": {
     "duration": 0.012548,
     "end_time": "2025-04-05T07:08:57.355007",
     "exception": false,
     "start_time": "2025-04-05T07:08:57.342459",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A basic residual block with two 3x3 convolutional layers and a ReLU activation.\"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class DenoiseSRNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A Super-Resolution network with denoising capabilities using residual learning and pixel shuffle upsampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3, num_features=64, num_res_blocks=12, upscale=4):\n",
    "        super().__init__()\n",
    "        \n",
    "       \n",
    "        self.entry = nn.Conv2d(in_channels, num_features, kernel_size=3, padding=1)\n",
    "        \n",
    "     \n",
    "        self.residual_layers = nn.Sequential(\n",
    "            *[ResidualBlock(num_features) for _ in range(num_res_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Upsampling\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(num_features, num_features * (upscale ** 2), kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(upscale),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output = nn.Conv2d(num_features, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry(x)\n",
    "        x = self.residual_layers(x)\n",
    "        x = self.upsample(x)\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12619bf3-3860-41ec-a865-d13ad2640cb2",
   "metadata": {},
   "source": [
    "## Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a420d",
   "metadata": {
    "papermill": {
     "duration": 0.010129,
     "end_time": "2025-04-05T07:08:57.367865",
     "exception": false,
     "start_time": "2025-04-05T07:08:57.357736",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CharbonnierLoss(nn.Module):\n",
    "   \n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return torch.mean(torch.sqrt((x - y) ** 2 + self.eps))\n",
    "\n",
    "\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg_features = vgg19(pretrained=True).features[:16].eval()\n",
    "        for param in vgg_features.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg_features\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        sr_features = self.vgg(sr)\n",
    "        hr_features = self.vgg(hr)\n",
    "        return self.criterion(sr_features, hr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381dc1ac",
   "metadata": {
    "papermill": {
     "duration": 0.013999,
     "end_time": "2025-04-05T07:08:57.384778",
     "exception": false,
     "start_time": "2025-04-05T07:08:57.370779",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, loss_fn):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for lr_imgs, hr_imgs in dataloader:\n",
    "        lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(lr_imgs)\n",
    "        loss = loss_fn(preds, hr_imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * lr_imgs.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def evaluate_psnr(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_psnr = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in dataloader:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "            preds = model(lr_imgs)\n",
    "\n",
    "            mse = F.mse_loss(preds, hr_imgs, reduction='none').mean(dim=[1, 2, 3])\n",
    "            psnr = 10 * torch.log10(1 / (mse + 1e-8))\n",
    "            total_psnr += psnr.sum().item()\n",
    "\n",
    "    return total_psnr / len(dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421992f-8d52-4e8c-8fab-d8721cdc3f77",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cd066-6b20-4120-9a33-b821c1e301c2",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(num_epochs=50, batch_size=4, lr=1e-3, \n",
    "                train_dir='/kaggle/input/dlp-jan-2025-nppe-3/archive/train',\n",
    "                val_dir='/kaggle/input/dlp-jan-2025-nppe-3/archive/val'):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset setup\n",
    "    train_dataset = LowLightSRDataset(\n",
    "        lr_dir=f\"{train_dir}/train\",\n",
    "        hr_dir=f\"{train_dir}/gt\",\n",
    "        transform_lr=T.ToTensor(),\n",
    "        transform_hr=T.ToTensor()\n",
    "    )\n",
    "    val_dataset = LowLightSRDataset(\n",
    "        lr_dir=f\"{val_dir}/val\",\n",
    "        hr_dir=f\"{val_dir}/gt\",\n",
    "        transform_lr=T.ToTensor(),\n",
    "        transform_hr=T.ToTensor()\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Model, optimizer, scheduler\n",
    "    model = DenoiseSRNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    # Loss Function\n",
    "    perceptual_loss = PerceptualLoss().to(device)\n",
    "    charbonnier_loss = CharbonnierLoss()\n",
    "    def loss_fn(pred, target):\n",
    "        return charbonnier_loss(pred, target) + 0.01 * perceptual_loss(pred, target)\n",
    "\n",
    "    # Training loop\n",
    "    best_psnr = 0.0\n",
    "    psnr_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device, loss_fn)\n",
    "        val_psnr = evaluate_psnr(model, val_loader, device)\n",
    "        psnr_history.append(val_psnr)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val PSNR: {val_psnr:.4f} dB\")\n",
    "\n",
    "        if val_psnr > best_psnr:\n",
    "            best_psnr = val_psnr\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    print(f\"\\nTraining complete. Best Validation PSNR: {best_psnr:.4f} dB\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f431f4-bb17-4baa-a4b6-6af6befddc0a",
   "metadata": {},
   "source": [
    "## Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34fdf74",
   "metadata": {
    "papermill": {
     "duration": 17092.354619,
     "end_time": "2025-04-05T11:53:49.741934",
     "exception": false,
     "start_time": "2025-04-05T07:08:57.387315",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#training the model \n",
    "model = train_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e61ace-4718-4c46-8d92-77fbd6765dfb",
   "metadata": {},
   "source": [
    "## Predicting on Test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94f2f4",
   "metadata": {
    "papermill": {
     "duration": 0.075802,
     "end_time": "2025-04-05T11:53:49.850745",
     "exception": false,
     "start_time": "2025-04-05T11:53:49.774943",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def inference(model_path, test_dir, output_dir, device=\"cuda\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load model\n",
    "    model = DenoiseSRNet()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # List test images\n",
    "    test_files = sorted(glob.glob(os.path.join(test_dir, \"*.png\")))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_path in test_files:\n",
    "            filename = os.path.basename(img_path)\n",
    "            lr_img = Image.open(img_path).convert(\"RGB\")\n",
    "            lr_tensor = F.to_tensor(lr_img).unsqueeze(0).to(device)\n",
    "            \n",
    "            sr_tensor = model(lr_tensor)\n",
    "            sr_img = sr_tensor.squeeze(0).cpu().clamp(0,1)\n",
    "            \n",
    "            # Convert to PIL\n",
    "            sr_img_pil = T.ToPILImage()(sr_img)\n",
    "            sr_img_pil.save(os.path.join(output_dir, filename))\n",
    "    \n",
    "    print(f\"predictions complete. Results saved in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac3efd",
   "metadata": {
    "papermill": {
     "duration": 15.969045,
     "end_time": "2025-04-05T11:54:05.828962",
     "exception": false,
     "start_time": "2025-04-05T11:53:49.859917",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#generating inf \n",
    "inference(\"/kaggle/working/best_model.pth\",\"/kaggle/input/dlp-jan-2025-nppe-3/archive/test\",\"results\",device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e920a10-45d9-44b2-9631-09974eabdbaa",
   "metadata": {},
   "source": [
    "## Generating Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f93999",
   "metadata": {
    "papermill": {
     "duration": 0.690982,
     "end_time": "2025-04-05T11:54:06.529867",
     "exception": false,
     "start_time": "2025-04-05T11:54:05.838885",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "def images_to_csv(folder_path, output_csv):\n",
    "    data_rows = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = Image.open(image_path).convert('L') \n",
    "            image_array = np.array(image).flatten()[::8]   \n",
    "            image_id = filename.split('.')[0].replace('test_', 'gt_') # Replace test with gt\n",
    "            data_rows.append([image_id, *image_array])\n",
    "    column_names = ['ID'] + [f'pixel_{i}' for i in range(len(data_rows[0]) - 1)]\n",
    "    df = pd.DataFrame(data_rows, columns=column_names)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f'Successfully saved to {output_csv}')\n",
    "    \n",
    "    \n",
    "folder_path = '/kaggle/working/results'\n",
    "output_csv = 'submission.csv'\n",
    "images_to_csv(folder_path, output_csv)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11639668,
     "sourceId": 97753,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17164.927079,
   "end_time": "2025-04-05T11:54:52.534010",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-05T07:08:47.606931",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
