{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setting random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_dir = \"/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data/train/images\"\n",
    "label_dir = \"/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data/train/labels\"\n",
    "test_dir = \"/kaggle/input/dlp-object-detection-week-10/final_dlp_data/final_dlp_data/test/images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Creating YOLO dataset structure\n",
    "dataset_path = \"/kaggle/working/mosquito-dataset\"\n",
    "os.makedirs(f\"{dataset_path}/images/train\", exist_ok=True)\n",
    "os.makedirs(f\"{dataset_path}/images/val\", exist_ok=True)\n",
    "os.makedirs(f\"{dataset_path}/labels/train\", exist_ok=True)\n",
    "os.makedirs(f\"{dataset_path}/labels/val\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Getting all images and split into train/val\n",
    "img_files = glob.glob(os.path.join(img_dir, \"*.jpeg\"))\n",
    "train_files, val_files = train_test_split(img_files, test_size=0.2, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to move image-label pairs\n",
    "def move_files(file_list, src_img_dir, src_label_dir, dest_img_dir, dest_label_dir):\n",
    "    for file in file_list:\n",
    "        filename = os.path.basename(file)\n",
    "        label_file = os.path.join(src_label_dir, filename.replace(\".jpeg\", \".txt\"))\n",
    "        \n",
    "        shutil.copy(file, os.path.join(dest_img_dir, filename))\n",
    "        if os.path.exists(label_file):\n",
    "            shutil.copy(label_file, os.path.join(dest_label_dir, filename.replace(\".jpeg\", \".txt\")))\n",
    "        else:\n",
    "            # Create an empty label file if none exists to avoid errors\n",
    "            with open(os.path.join(dest_label_dir, filename.replace(\".jpeg\", \".txt\")), 'w') as f:\n",
    "                pass\n",
    "\n",
    "# Moving files\n",
    "move_files(train_files, img_dir, label_dir, f\"{dataset_path}/images/train\", f\"{dataset_path}/labels/train\")\n",
    "move_files(val_files, img_dir, label_dir, f\"{dataset_path}/images/val\", f\"{dataset_path}/labels/val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Printing dataset statistics\n",
    "print(f\"Total images: {len(img_files)}\")\n",
    "print(f\"Training images: {len(train_files)}\")\n",
    "print(f\"Validation images: {len(val_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Creating Dataset YAML File\n",
    "dataset_yaml = f\"\"\"\n",
    "path: {dataset_path}\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: {test_dir}\n",
    "\n",
    "nc: 6\n",
    "names: [\"aegypti\", \"albopictus\", \"anopheles\", \"culex\", \"culiseta\", \"japonicus/koreicus\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{dataset_path}/dataset.yaml\", \"w\") as f:\n",
    "    f.write(dataset_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading a pretrained model\n",
    "model = YOLO('yolov8m.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training with optimized parameters for this task\n",
    "results = model.train(\n",
    "    data=f\"{dataset_path}/dataset.yaml\", \n",
    "    epochs=50,              \n",
    "    batch=16,                  \n",
    "    imgsz=512,               \n",
    "    seed=SEED,\n",
    "    patience=5,             \n",
    "    optimizer=\"AdamW\",       # AdamW optimizer (better generalization)\n",
    "    lr0=0.0005,               # Learning rate\n",
    "    weight_decay=0.0005,     # Regularization\n",
    "    flipud=0.0,              # No vertical flipping (mosquito orientation matters)\n",
    "    fliplr=0.5,              # Horizontal flip augmentation\n",
    "    mosaic=0.8,              # Mosaic augmentation\n",
    "    mixup=0.1,               # Mixup augmentation\n",
    "    copy_paste=0.1,          # Copy-paste augmentation\n",
    "    workers=4,               \n",
    "    cache=True,              # Caching dataset for faster training\n",
    "    verbose=False,           \n",
    "    save=True,\n",
    "    device=\"cuda\"            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Validating the model to check performance\n",
    "metrics = model.val()\n",
    "print(f\"mAP@0.5: {metrics.box.map50}, mAP@0.5:0.95: {metrics.box.map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on Test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = model.predict(\n",
    "    source=test_dir, \n",
    "    save_txt=True, \n",
    "    save_conf=True,\n",
    "    conf=0.1,                # Confidence threshold for predictions\n",
    "    iou=0.5,                 # IoU threshold for NMS\n",
    "    max_det=300,               # Maximum detections per image\n",
    "    agnostic_nms=True,        # Class-agnostic NMS\n",
    "    augment=True,            \n",
    "    visualize=True,         \n",
    "    device=\"cuda\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Getting the directory where predictions were saved\n",
    "predict_dir = model.predictor.save_dir\n",
    "pred_dir = os.path.join(predict_dir, \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Defining class label mapping\n",
    "class_labels = {\n",
    "    \"0\": \"aegypti\",\n",
    "    \"1\": \"albopictus\",\n",
    "    \"2\": \"anopheles\",\n",
    "    \"3\": \"culex\",\n",
    "    \"4\": \"culiseta\",\n",
    "    \"5\": \"japonicus/koreicus\"\n",
    "}\n",
    "\n",
    "# Function to get actual image dimensions\n",
    "def get_image_size(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is not None:\n",
    "        img_height, img_width, _ = img.shape\n",
    "        return img_width, img_height\n",
    "    else:\n",
    "        print(f\"Warning: Unable to read {image_path}\")\n",
    "        return None, None\n",
    "\n",
    "# Getting all test image files with full filenames (including .jpeg extension)\n",
    "test_image_files = glob.glob(os.path.join(test_dir, \"*.jpeg\"))\n",
    "test_image_filenames = [os.path.basename(file) for file in test_image_files]\n",
    "test_image_filenames.sort()  # Sort for consistent ordering\n",
    "\n",
    "submission_data = []\n",
    "\n",
    "for idx, img_filename in enumerate(test_image_filenames):\n",
    "    img_id = img_filename.replace(\".jpeg\", \"\")  \n",
    "    pred_file_path = os.path.join(pred_dir, f\"{img_id}.txt\")\n",
    "    img_path = os.path.join(test_dir, img_filename)\n",
    "    \n",
    "    img_width, img_height = get_image_size(img_path)  # Getting actual image size\n",
    "    \n",
    "    if img_width is None or img_height is None:\n",
    "        print(f\"Skipping {img_filename} due to missing image dimensions.\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.exists(pred_file_path) and os.path.getsize(pred_file_path) > 0:\n",
    "        detections = []\n",
    "        with open(pred_file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 6:  # Class, coordinates, and confidence\n",
    "                    class_id = parts[0]\n",
    "                    x_center, y_center, width, height = map(float, parts[1:5])\n",
    "                    confidence = float(parts[5])\n",
    "\n",
    "                    # Converting normalized coordinates to pixel values\n",
    "                    x_center = float(parts[1])  \n",
    "                    y_center = float(parts[2])\n",
    "                    width = float(parts[3])\n",
    "                    height = float(parts[4])\n",
    "\n",
    "\n",
    "                    detections.append({\n",
    "                        \"class\": class_id,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"x_center\": x_center,\n",
    "                        \"y_center\": y_center,\n",
    "                        \"width\": width,\n",
    "                        \"height\": height\n",
    "                    })\n",
    "        \n",
    "        if detections:\n",
    "            # Sort by confidence (highest first)\n",
    "            detections.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
    "            best_detection = detections[0]\n",
    "            \n",
    "            submission_data.append({\n",
    "                \"id\": idx,\n",
    "                \"ImageID\": img_filename,\n",
    "                \"LabelName\": class_labels.get(best_detection[\"class\"], \"albopictus\"),\n",
    "                \"Conf\": best_detection[\"confidence\"],\n",
    "                \"xcenter\": best_detection[\"x_center\"],\n",
    "                \"ycenter\": best_detection[\"y_center\"],\n",
    "                \"bbx_width\": best_detection[\"width\"],\n",
    "                \"bbx_height\": best_detection[\"height\"]\n",
    "            })\n",
    "        else:\n",
    "            \n",
    "            submission_data.append({\n",
    "                \"id\": idx,\n",
    "                \"ImageID\": img_filename,\n",
    "                \"LabelName\": \"albopictus\",\n",
    "                \"Conf\": 0.5,  # Default confidence\n",
    "                \"xcenter\": 0.5,\n",
    "                \"ycenter\": 0.5,\n",
    "                \"bbx_width\": 0.2,\n",
    "                \"bbx_height\": 0.2\n",
    "            })\n",
    "    else:\n",
    "        \n",
    "        submission_data.append({\n",
    "            \"id\": idx,\n",
    "            \"ImageID\": img_filename,\n",
    "            \"LabelName\": \"albopictus\",\n",
    "            \"Conf\": 0.5,\n",
    "            \"xcenter\": 0.5,\n",
    "            \"ycenter\": 0.5,\n",
    "            \"bbx_width\": 0.2,\n",
    "            \"bbx_height\": 0.2\n",
    "        })\n",
    "\n",
    "# Creating submission dataframe\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "# Loading sample submission as template\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/dlp-object-detection-week-10/sample_submission.csv\")\n",
    "\n",
    "# Mapping predictions to sample submission format\n",
    "prediction_map = {row[\"ImageID\"]: row for _, row in submission_df.iterrows()}\n",
    "\n",
    "final_submission = sample_submission.copy()\n",
    "\n",
    "for i, row in final_submission.iterrows():\n",
    "    img_id = row['ImageID']\n",
    "    if img_id in prediction_map:\n",
    "        final_submission.at[i, 'LabelName'] = prediction_map[img_id]['LabelName']\n",
    "        final_submission.at[i, 'Conf'] = prediction_map[img_id]['Conf']\n",
    "        final_submission.at[i, 'xcenter'] = prediction_map[img_id]['xcenter']\n",
    "        final_submission.at[i, 'ycenter'] = prediction_map[img_id]['ycenter']\n",
    "        final_submission.at[i, 'bbx_width'] = prediction_map[img_id]['bbx_width']\n",
    "        final_submission.at[i, 'bbx_height'] = prediction_map[img_id]['bbx_height']\n",
    "\n",
    "\n",
    "expected_columns = ['id', 'ImageID', 'LabelName', 'Conf', 'xcenter', 'ycenter', 'bbx_width', 'bbx_height']\n",
    "final_submission = final_submission[expected_columns]\n",
    "\n",
    "# Saving the final submission\n",
    "final_submission.to_csv(\"/kaggle/working/21F3000728.csv\", index=False)\n",
    "print(f\"Final submission saved with {len(final_submission)} entries.\")\n",
    "print(\"Sample of submission format:\")\n",
    "print(final_submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11404671,
     "sourceId": 96038,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
